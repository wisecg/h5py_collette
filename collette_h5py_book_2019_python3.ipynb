{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from timeit import timeit\n",
    "import posixpath\n",
    "from functools import partial\n",
    "import time, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch. 1 & 2: Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st example\n",
    "temps = np.random.random(1024) # time series data as array (our dataset)\n",
    "dt_temp = 10.0 # sampling rate\n",
    "t_start = 1375204299\n",
    "station = 15\n",
    "winds = np.random.random(2048) # second dataset\n",
    "dt_wind = 5.0 # different sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"weather.h5\",\"w\") # careful, 'w' mode overwrites existing file\n",
    "f[\"/15/temperature\"] = temps # dataset\n",
    "f[\"/15/temperature\"].attrs[\"dt\"] = dt_temp # attributes (dict style)\n",
    "f[\"/15/temperature\"].attrs[\"t_start\"] = t_start\n",
    "f[\"/15/winds\"] = winds\n",
    "f[\"/15/winds\"].attrs[\"dt\"] = dt_wind\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we declare the output file. Option \"a\" is the default.\n",
    "\n",
    "(Read/write if exists, create otherwise, but don't overwrite anything)  \n",
    "\n",
    "Once we've run the above block, we can't run it again, b/c things would be overwritten.\n",
    "\n",
    "Also note, groups are created by using the \"/\", or we could have used `f.create_group(\"name\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"weather.h5\")\n",
    "print(f.keys()) # this should almost be called \"groups\"\n",
    "print(f[\"/15\"].keys()) # give the subgroups\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"context manager\" - closes file when the task is done\n",
    "with h5py.File(\"weather.h5\") as f:\n",
    "    print(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5py has a funny way of iterating through groups,\n",
    "# you have to pass 'visititems' a function\n",
    "def print_attrs(name, obj):\n",
    "    print(name)\n",
    "    for key, val in obj.attrs.items():\n",
    "        print(\"    %s: %s\" % (key, val))\n",
    "\n",
    "f = h5py.File('weather.h5','r')\n",
    "f.visititems(print_attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at a few different \"file drivers\".\n",
    "\n",
    "NOTE: In this notebook I am using \"w\" a lot because I don't care about deleting/recreating these files.  \n",
    "\n",
    "Check this: http://docs.h5py.org/en/stable/high/file.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default: auto for the system\n",
    "f = h5py.File(\"t1.h5\",\"w\") \n",
    "\n",
    "# load the file into memory\n",
    "f = h5py.File(\"t2.h5\",\"w\", driver=\"core\") \n",
    "\n",
    "# load into memory, but create an on-disk file where\n",
    "# the file image is saved when closed.\n",
    "# also tells hdf5 to load any existing image from disk\n",
    "# when you open the file.\n",
    "f = h5py.File(\"t3.h5\",\"w\", driver=\"core\", backing_store=True) \n",
    "\n",
    "# split a file into multiple \"images\" w/ a declared max size.\n",
    "# not working here for some reason.\n",
    "# f = h5py.File(\"t4.h5\",\"w\", driver=\"family\", memb_size=1024**3)\n",
    "\n",
    "# mpio driver for Parallel HDF5 (see ch 9)\n",
    "# f = h5py.File(\"t5.h5\", \"w\", driver='mpio')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5 files have a \"user block\" @ the beginning, where you\n",
    "# can store arbitrary data.  You can't have the file open\n",
    "# in HDF5 while writing to the user block.\n",
    "f = h5py.File(\"userblock.h5\", \"w\", userblock_size=512)\n",
    "\n",
    "with open(\"userblock.h5\", \"r+\") as f: # open as regular python file\n",
    "    # f.write(\"a\"*512)\n",
    "    f.write(\"oh hai i'm clint\")\n",
    "    \n",
    "# i can't figure out how to actually READ it though ...\n",
    "with open(\"userblock.h5\") as f:\n",
    "    f.readline() # gives an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Working With Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset basics\n",
    "with h5py.File(\"test.h5\", \"w\") as f:\n",
    "    arr = np.ones((5,2))\n",
    "    f[\"my dataset\"] = arr\n",
    "    \n",
    "    # this is a \"proxy\" object that lets you read/write to the\n",
    "    # underlying HDF5 dataset on disk.  \n",
    "    # but it's LIKE a numpy array in many respects.\n",
    "    dset = f[\"my dataset\"]\n",
    "    print(dset)\n",
    "    print(dset.dtype)\n",
    "    print(dset.shape)\n",
    "    \n",
    "    # read the dataset into a np array w/ the Ellipsis.\n",
    "    # another way of saying it:\n",
    "    # \"slicing INTO a dataset object returns a numpy array\"\n",
    "    # this is where HDF5 actually reads the data from disk.\n",
    "    out = dset[...]\n",
    "    print(out)\n",
    "    print(type(out))\n",
    "    \n",
    "    # update a portion of the dataset\n",
    "    dset[1:4, 1] = 2.0\n",
    "    print(dset[...])\n",
    "    \n",
    "    # create some empty datasets\n",
    "    dset2 = f.create_dataset(\"test2\", (10, 10))\n",
    "    dset3 = f.create_dataset(\"test3\", (10, 10), dtype=np.complex64)\n",
    "    print(dset3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an empty dataset that is really big (preallocated)\n",
    "# and only write to a small part of it.\n",
    "\n",
    "# NOTE: the book is outdated, apparently.\n",
    "# you have to turn on chunking for h5py\n",
    "# to not allocate space for the entire file.\n",
    "# https://stackoverflow.com/questions/45145389/size-on-disk-of-a-partly-filled-hdf5-dataset\n",
    "f = h5py.File(\"test.h5\",\"w\")\n",
    "\n",
    "# auto-determine chunk size\n",
    "# ds = f.create_dataset(\"bigds\", (1024**3,), dtype=np.float32, chunks=True)\n",
    "\n",
    "# set chunk size manually (16384 floats = 64 KB)\n",
    "ds = f.create_dataset(\"bigds\", (1024**3,), dtype=np.float32, chunks=(2**14,) )\n",
    "print(ds.chunks)\n",
    "\n",
    "# chunk size has performance implications. \n",
    "# It’s recommended to keep the total size of your chunks \n",
    "# between 10 KiB and 1 MiB, larger for larger datasets. \n",
    "# Also keep in mind that when any element in a chunk is \n",
    "# accessed, the entire chunk is read from disk.\n",
    "\n",
    "ds[0:1024] = np.arange(1024)\n",
    "f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use float64s in memory to minimize rounding error, \n",
    "# but store as float32s to save space\n",
    "bigdata = np.ones((100,1000))\n",
    "# print(bigdata.dtype, bigdata.shape)\n",
    "\n",
    "with h5py.File(\"big1.h5\",'w') as f:\n",
    "    # f[\"big\"] = bigdata # 783K\n",
    "    f.create_dataset(\"big\", data=bigdata, dtype=np.float32) # 393K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "ls -lh big1.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we had a single-precision float dataset \n",
    "on disk, but wanted to read it in as double-precision,\n",
    "we would have to be careful b/c it might be more than memory\n",
    "or we might want to do the type conversion on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_out = np.empty((100,1000), dtype=np.float64)\n",
    "\n",
    "with h5py.File(\"big1.h5\") as f:\n",
    "    dset = f[\"big\"]\n",
    "    \n",
    "    # method 1: preallocated numpy array of the DESIRED type.  \n",
    "    dset.read_direct(big_out)\n",
    "    \n",
    "    # method 2 : read with \"astype\" - hdf5 converts on the fly\n",
    "    with dset.astype('float64'):\n",
    "        out = dset[0,:] # read by slicing\n",
    "    \n",
    "print(big_out[:3,:3], big_out.dtype, out.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# be careful about how you slice!  \n",
    "# it can impact performance.\n",
    "\n",
    "with h5py.File(\"big1.h5\") as f:\n",
    "    dset = f['big']\n",
    "    out = dset[0:10, 20:70]\n",
    "    \n",
    "f = h5py.File(\"big1.h5\")\n",
    "dset = f[\"big\"]\n",
    "\n",
    "# # the slow way (> 10 seconds!)\n",
    "# for ix in range(100):\n",
    "#     for iy in range(1000):\n",
    "#         val = dset[ix, iy]\n",
    "#         if val < 0 : \n",
    "#             dset[ix, iy] = 0 # trim \n",
    "\n",
    "# the fast way (done ~ instantly)\n",
    "for ix in range(100):\n",
    "    val = dset[ix, :] # read a whole row into ndarray\n",
    "    val[ val < 0 ] = 0 # trim\n",
    "    dset[ix, :] = val # write back to file\n",
    "            \n",
    "f.close()\n",
    "\n",
    "# moral of the story: \n",
    "# writing to a Dataset one element (or even a few elements)\n",
    "# at a time, is a great way to get poor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"test2.h5\", \"w\")\n",
    "\n",
    "# multidimensional slicing\n",
    "dset = f.create_dataset('4d', shape=(100,80,50,20))\n",
    "print(dset[90,...,0].shape)\n",
    "print(dset[90,...].shape)\n",
    "\n",
    "# handling scalar datasets\n",
    "\n",
    "# method 1 - a 1d numpy array\n",
    "dset = f.create_dataset('1d', shape=(1,), data=42)\n",
    "print(dset.shape)\n",
    "print(dset[0], dset[...]) # note the different outputs\n",
    "\n",
    "# method 2 - empty tuple, shape is (), indexing doesn't work\n",
    "dset = f.create_dataset('0d', data=42)\n",
    "print(dset.shape)\n",
    "# print(dset[0]) # fails\n",
    "print(dset[...], type(dset[...])) # this returns an ARRAY \n",
    "print(dset[()], type(dset[()])) # this wacky thing gives you the value\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boolean indexing is great for np arrays, `idx = np.where(blahblahblah)` and we can also do it directly on HDF5 Dataset objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random(10)*2 - 1\n",
    "print(data.shape)\n",
    "\n",
    "f = h5py.File(\"test2.h5\", \"w\")\n",
    "ds = f.create_dataset(\"randoms\", data=data)\n",
    "\n",
    "# do the indexing on the dataset, using the np array\n",
    "ds[data < 0] = 0 \n",
    "print(ds[...])\n",
    "\n",
    "# note from the book:\n",
    "# for very large indexing expressions with lots of True values,\n",
    "# it may be faster to, for example, modify the data on the Python\n",
    "# side and write the dataset o\n",
    "ut again.  If you suspect a slowdown\n",
    "# it's a good idea to test this.\n",
    "\n",
    "# modify some elements in place (array is same length as slice)\n",
    "ds[data < 0] = -1 * data[data<0]\n",
    "print(ds[...])\n",
    "\n",
    "# can also specify given coordinate lists.\n",
    "# NOTE: this is much more efficient than Boolean masking \n",
    "# for large datasets\n",
    "print(ds[[1,2,7]])\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic broadcasting\n",
    "\n",
    "# when we make slicing assignment where the number of elements\n",
    "# on the LHS and RHS are not equal, numpy and hdf5 use \"broadcasting\":\n",
    "# dset[data<0] = 0\n",
    "# \"used judiciously, it can give your application a performance boost\"\n",
    "\n",
    "f = h5py.File(\"big1.h5\")\n",
    "dset = f[\"big\"]\n",
    "\n",
    "# suppose we want to copy the trace at dset[0, :], \n",
    "# and overwrite all the others in the file.\n",
    "\n",
    "# method 1 (slow)\n",
    "# we have to write the loop, get the bc's right, and perform 100 slices\n",
    "data = dset[0,:]\n",
    "for i in range(100):\n",
    "    dset[i,:] = data\n",
    "    \n",
    "# method 2 (fast, w/broadcasting)\n",
    "dset[:,:] = dset[0,:]\n",
    "\n",
    "# RHS is (1000,) LHS is (100,1000).\n",
    "# Since the last dimensions match, h5py repeatedly copies the data\n",
    "# across all 100 remaining indies. \n",
    "# There's only one slicing operation!\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading directly into an existing array\n",
    "# automatically performs type conversion\n",
    "\n",
    "f = h5py.File(\"big1.h5\")\n",
    "dset = f['big']\n",
    "\n",
    "# ex. 1 (repeated) - read float32 data into a float64 ndarray\n",
    "print(dset.dtype)\n",
    "out = np.empty((100,1000), dtype=np.float64)\n",
    "dset.read_direct(out)\n",
    "\n",
    "# ex. 2\n",
    "# suppose we wanted to read the first time trace [0,:]\n",
    "# and deposit it into the out array at out[50,:]\n",
    "# we can select both the source and destination:\n",
    "dset.read_direct(out, source_sel=np.s_[0,:], dest_sel=np.s_[50,:])\n",
    "\n",
    "# here np._s is a thing that takes slices and returns a np \"slice\" object\n",
    "\n",
    "# you don't have to match the shape of the output array to the dataset:\n",
    "\n",
    "# method 1:\n",
    "out = dset[:,0:50]\n",
    "means = out.mean(axis=1)\n",
    "\n",
    "# method 2, with read_direct:\n",
    "# this is faster when the shapes of the arrays are really huge\n",
    "out = np.empty((100,50), dtype=np.float32)\n",
    "dset.read_direct(out, np.s_[:,0:50]) # dest_sel can be omitted\n",
    "means = out.mean(axis=1)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"endian-ness\" : relating to how multi-byte numbers are represented.\n",
    "# if we wanted to store a 4-byte floating point number:\n",
    "# little-endian : stores the LEAST significant byte first\n",
    "# big-endian : stores the MOST significant byte first\n",
    "\n",
    "a = np.ones((1000,1000), dtype='<f4') # little-endian 4-byte float\n",
    "b = np.ones((1000,1000), dtype='>f4') # big-endian 4-byte float\n",
    "\n",
    "%timeit a.mean\n",
    "%timeit b.mean\n",
    "\n",
    "# huh.  the book says big-endian is a factor 2 slower, \n",
    "# but they're pretty much identical here. \n",
    "# i guess stuff's been updated\n",
    "\n",
    "# to convert to \"native\" endian-ness for your system:\n",
    "# 1. use read_direct w/ a natively formatted array you create yourself\n",
    "# 2. use the 'astype' context manager\n",
    "# 3. convert the numpy array in place:\n",
    "c = b.view(\"float32\")\n",
    "c[:] = b\n",
    "b = c\n",
    "%timeit b.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizing datasets\n",
    "\n",
    "# datasets have a SHAPE and TYPE.  \n",
    "# TYPE is unchangeable, but SHAPE is not.\n",
    "\n",
    "f = h5py.File(\"test3.h5\",\"w\")\n",
    "\n",
    "dset = f.create_dataset('fixed', (2,2)) # a 4-element dataset\n",
    "print(dset.shape, dset.maxshape)\n",
    "\n",
    "# huh.  so we have to set dset.maxshape\n",
    "dset = f.create_dataset('resizable', (2,2), maxshape=(2,2))\n",
    "dset.resize((1,1))\n",
    "print(dset.shape)\n",
    "\n",
    "# what if you don't know when you create the dataset how big it should be?\n",
    "# like maybe during data acquisition from a digitizer?\n",
    "dset = f.create_dataset('unlimited', (2,2), maxshape=(2,None))\n",
    "dset.resize((2, 2**30))\n",
    "print(dset.shape)\n",
    "\n",
    "# note that you can mark as many axes you want as unlimited.\n",
    "\n",
    "# but you CAN'T change the total number of axes:\n",
    "# dset.resize((2,2,2)) # throws error\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling data w/ resize\n",
    "\n",
    "print(\"numpy side:\")\n",
    "a = np.array([[1,2],[3,4]])\n",
    "print(a)\n",
    "a.resize((1,4))\n",
    "print(a)\n",
    "a.resize((1,10)) # new elements are initialized to 0\n",
    "print(a)\n",
    "\n",
    "# hdf5 side: no reshuffling is ever performed\n",
    "\n",
    "f = h5py.File(\"test3.h5\",\"w\")\n",
    "\n",
    "dset = f.create_dataset('sizetest', (2,2), dtype=np.int32, maxshape=(None,None))\n",
    "\n",
    "print(\"hdf5 side\")\n",
    "dset[...] = [[1,2],[3,4]]\n",
    "dset.resize((1,4))\n",
    "print(dset[...])\n",
    "dset.resize((1,10))\n",
    "print(dset[...])\n",
    "print(\"whoops, applying numpy resizing to datasets caused us to lose data!\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending to a dataset with \"resize\"\n",
    "\n",
    "f = h5py.File(\"test3.h5\", \"w\")\n",
    "\n",
    "dset1 = f.create_dataset(\"timetraces\", (1,1000), maxshape=(None, 1000))\n",
    "\n",
    "def add_trace_1(arr):\n",
    "    # every time a new 1000-element array is added,\n",
    "    # the dataset is simply expanded by a single entry\n",
    "    dset1.resize((dset1.shape[0]+1, 1000))\n",
    "    dset1[-1,:] = arr\n",
    "    \n",
    "# be careful, if the number of resize calls is equal to the\n",
    "# number of insertions, this doesn't scale well.\n",
    "\n",
    "# alternatively, keep track of the # insertions and then \"trim\"\n",
    "# the dataset when done:\n",
    "\n",
    "dset2 = f.create_dataset('timetraces2', (5000,1000), maxshape=(None,1000))\n",
    "\n",
    "ntraces = 0\n",
    "def add_trace_2(arr):\n",
    "    global ntraces\n",
    "    dset2[ntraces,:] = arr\n",
    "    ntraces += 1\n",
    "\n",
    "def done():\n",
    "    dset2.resize((ntraces,1000))\n",
    "\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Chunking and Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, all but the smallest HDF5 datasets use contiguous storage.\n",
    "The data in your DS is flattened to disk using the same rules as numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider a set of 100 images:\n",
    "f = h5py.File(\"imagetest.h5\",\"w\")\n",
    "\n",
    "dset = f.create_dataset(\"Images\", (100,480,640), dtype='uint8')\n",
    "\n",
    "# a contiguous dataset stores the images on disk,\n",
    "# one 640-element \"scanline\" after another\n",
    "\n",
    "image = dset[0,:,:]\n",
    "print(image.shape)\n",
    "\n",
    "# what if we wanted to only access the first 64x64 tile?\n",
    "tile = dset[0,0:64,0:64]\n",
    "\n",
    "# if we looped over the images, we wouldn't be able to access\n",
    "# the tiles as a contiguous block, which is slow.\n",
    "# %timeit dset[:,0:64,0:64]\n",
    "\n",
    "# a better approach: use chunking\n",
    "dset = f.create_dataset(\"chunked\", (100,480,640), dtype=\"i1\", chunks=(1,64,64))\n",
    "print(dset.chunks)\n",
    "# %timeit dset[:, 0:64, 0:64]\n",
    "\n",
    "# auto chunking w/o specifying chunk size\n",
    "dset = f.create_dataset(\"Images2\", (100,480,640), \"f\", chunks=True)\n",
    "print(dset.chunks)\n",
    "\n",
    "# NOTE: chunks bigger than 1MB can't be loaded into the fast cache\n",
    "\n",
    "# TIP: you should only do chunking in cases when you know for sure\n",
    "# your dataset will be accessed in a way that's likely to be inefficient\n",
    "# with either contiguous storage or an auto-guessed chunk shape.\n",
    "\n",
    "# for my stuff, maybe a good \"chunk\" would be a waveform, \n",
    "# i.e. chunks=(1,2016) or whatever it is\n",
    "# OR, maybe it would be the pygama \"block size\", so chunks=(3000,2016),\n",
    "# but maybe that would be more than 1MB.  idk.  try it out sometime.\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizing datasets (revisiting chunking)\n",
    "\n",
    "f = h5py.File(\"resizing.h5\", \"w\")\n",
    "\n",
    "# declare two datasets, both expandable, w/ differing initial sizes\n",
    "dset1 = f.create_dataset(\"timetraces1\", (1, 1000), maxshape=(None, 1000))\n",
    "dset2 = f.create_dataset(\"timetraces2\", (5000, 1000), maxshape=(None, 1000))\n",
    "\n",
    "def add_trace_1(arr):\n",
    "    # Add a trace to the ds, expanding it as necessary\n",
    "    dset1.resize((dset1.shape[0]+1, 1000))\n",
    "    dset1[-1,:] = arr\n",
    "    \n",
    "ntraces = 0\n",
    "def add_trace_2(arr):\n",
    "    # Add a trace to the ds, keeping count of the # traces written\n",
    "    global ntraces\n",
    "    dset2[ntraces,:] = arr\n",
    "    ntraces +=1\n",
    "    \n",
    "def done():\n",
    "    # after all calls to add_trace_2, trim the ds to size\n",
    "    dset2.resize((ntraces,1000))\n",
    "    \n",
    "def setup():\n",
    "    # re-init both datasets for the tests\n",
    "    global data, N, dset1, dset2, traces\n",
    "    data = np.random.random(1000)\n",
    "    N = 10000 # num iterations\n",
    "    dset1.resize((1,1000))\n",
    "    dset2.resize((10001,1000))\n",
    "    ntraces = 0\n",
    "    \n",
    "def test1():\n",
    "    # add N traces to the first ds\n",
    "    for ix in range(N):\n",
    "        add_trace_1(data)\n",
    "        \n",
    "def test2():\n",
    "    # add N traces to the second ds, then trim it\n",
    "    for ix in range(N):\n",
    "        add_trace_2(data)\n",
    "    done()\n",
    "\n",
    "# different from the book again.  method 2 is faster for me\n",
    "# print(timeit(test1, setup=setup, number=1)) # 1.89\n",
    "# print(timeit(test2, setup=setup, number=1)) # 1.69\n",
    "\n",
    "# print(dset1.chunks) # same as book\n",
    "# print(dset2.chunks) # different from book\n",
    "\n",
    "# so chunk shape is partly determined by the INITIAL SIZE of the dataset\n",
    "\n",
    "# try it again with a manual chunk shape:\n",
    "\n",
    "dset1 = f.create_dataset(\"timetraces3\", (1,1000), maxshape=(None,1000), chunks=(1,1000))\n",
    "dset2 = f.create_dataset(\"timetraces4\", (5000,1000), maxshape=(None,1000), chunks=(1,1000))\n",
    "\n",
    "# ok, method 2 is faster again, as it should be\n",
    "print(timeit(test1, setup=setup, number=1)) # 1.83\n",
    "print(timeit(test2, setup=setup, number=1)) # 1.55\n",
    "\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters and compression\n",
    "# the filters work on CHUNKS in hdf5\n",
    "\n",
    "f = h5py.File(\"filters.h5\",\"w\")\n",
    "\n",
    "# gzip compression filter\n",
    "dset = f.create_dataset(\"BigDataset\",(1000,1000),dtype='f',compression='gzip')\n",
    "print(dset.compression)\n",
    "print(dset.compression_opts) # for gzip, this is compression level\n",
    "print(dset.chunks) # 63*125*(4 bytes) = 30 KiB blocks for the compressor\n",
    "\n",
    "# compression is transparent (you don't notice it)\n",
    "dset[...] = 42.0\n",
    "print(dset[0,0])\n",
    "\n",
    "# other lossless compressors are LZF (python only), SZIP (proprietary, don't use)\n",
    "# also BLOSC, BZIP2, etc.\n",
    "# \"it's also rare for an application to spend most of its time compressing\n",
    "# or decompressing data, so try not to get carried away with speed testing.\"\n",
    "\n",
    "# SHUFFLE filter: use in conjunction w/ gzip to improve compression\n",
    "# for many datasets, most of the entropy occurs in a few bytes\n",
    "\n",
    "# FLETCHER32 filter: creates a checksum for each chunk, recorded in the\n",
    "# chunk's metadata.  When the chunk is read, we compare to the checksum\n",
    "# to check for any losses.\n",
    "\n",
    "# NOTE:  I would guess that for LEGEND/pygama data, we would want:\n",
    "# gzip + shuffle + fletcher32\n",
    "dset = f.create_dataset(\"Data\",(1000,), dtype='f', compression=\"gzip\", shuffle=True, fletcher32=True)\n",
    "# (and we might also want SWMR mode, but I haven't gotten to that yet)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5: Groups, Links, Iteration\n",
    "\n",
    "REMEMBER: _Groups work mostly like dictionaries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"groups.h5\",\"w\")\n",
    "\n",
    "subg = f.create_group(\"subgroup\")\n",
    "print(subg)\n",
    "print(subg.name)\n",
    "\n",
    "# can nest\n",
    "subsubg = subg.create_group(\"another_group\")\n",
    "print(subsubg.name)\n",
    "\n",
    "# create a bunch at once\n",
    "out = f.create_group('/some/big/path')\n",
    "print(out)\n",
    "\n",
    "# add some more groups w/ very simple datasets\n",
    "f[\"Dataset1\"] = 1.0\n",
    "f[\"Dataset2\"] = 2.0\n",
    "f[\"Dataset3\"] = 3.0\n",
    "subg[\"Dataset4\"] = 4.0\n",
    "\n",
    "# access them\n",
    "dset4 = f[\"subgroup\"][\"Dataset4\"] # works, but inefficient\n",
    "dset4 = f[\"subgroup/Dataset4\"] # right\n",
    "\n",
    "# use the `get` method if you're not sure if a group exists\n",
    "# this is an alternative to try/except'ing everything\n",
    "out = f.get(\"BadName\")\n",
    "print(out)\n",
    "\n",
    "# take the length of the group\n",
    "# (the number of objects DIRECTLY attached to the group)\n",
    "print(len(f))\n",
    "print(len(f[\"subgroup\"]))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve a File object for the file in which your obj resides:\n",
    "f = h5py.File(\"propdemo.h5\",\"w\")\n",
    "grp = f.create_group(\"hello\")\n",
    "print(grp.file == f)\n",
    "print(grp.parent) # return the Group obj that contains this object\n",
    "\n",
    "# can use this to check if a file is read/write, or just get the filename\n",
    "print(grp.file) # this annoyingly doesn't return a str\n",
    "print(f.filename) # this does\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### working with links\n",
    "\n",
    "There is a LAYER between the GROUP object \n",
    "and the OBJECTS that are its members.  \n",
    "The two are related by LINKS, which can be \"hard\", \"soft\", or \"external\".\n",
    "\n",
    "When you assign an object to a name in a group,\n",
    "that address (in memory) is recorded in the group \n",
    "and associated with the name you provided to form a link.\n",
    "\n",
    "This means objects in an hdf5 file can have more than one name!\n",
    "They can have _as many names_ as they have _links pointing to them_.\n",
    "The number of links that point to an object is recorded, and when no more links exist, the space used for the object is freed.  \n",
    "\n",
    "This kind of a link is called a _hard link_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate the multiple-name behavior of hard links\n",
    "\n",
    "f = h5py.File(\"linksdemo.h5\",\"w\")\n",
    "grpx = f.create_group('x')\n",
    "print(grpx.name)\n",
    "\n",
    "# create a second link, pointing to the group\n",
    "f['y'] = grpx\n",
    "\n",
    "# retrieve an object from location /y:\n",
    "grpy = f['y']\n",
    "print(grpy == grpx)\n",
    "\n",
    "# check names \n",
    "# (note, the one object in the file doesn't have a unique name!)\n",
    "print(grpx.name)\n",
    "print(grpy.name)\n",
    "\n",
    "# can also create an object w/ no name\n",
    "grpz = f.create_group(None)\n",
    "print(grpz.name)\n",
    "\n",
    "# there's no way to get to this group from the root group.\n",
    "# if we got rid of the python object grpz, the group would\n",
    "# be deleted and the space in the file reclaimed.\n",
    "# to avoid this, we can link the group into the file structure\n",
    "# (after creating it)\n",
    "f['z'] = grpz\n",
    "print(grpz.name)\n",
    "\n",
    "# delete a link \n",
    "# when you delete the last link to an object, it's destroyed\n",
    "del f['y']\n",
    "del f['x']\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# if you're deleting objects inside the hdf5 file, \n",
    "# you might wind up with free space.  \n",
    "# HDF5 provides the CLT \"h5repack\"\n",
    "h5repack linksdemo.h5 repacked.h5\n",
    "ls -lh linksdemo.h5 repacked.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft links demo\n",
    "\n",
    "# these store the PATH to an object\n",
    "\n",
    "f = h5py.File(\"test.h5\", \"w\")\n",
    "\n",
    "grp = f.create_group(\"mygroup\")\n",
    "dset = grp.create_dataset(\"dataset\", (100,))\n",
    "\n",
    "f['hardlink'] = dset\n",
    "print(f['hardlink'] == grp['dataset'])\n",
    "\n",
    "# hard links always point to their object\n",
    "grp.move('dataset', \"new_dataset_name\")\n",
    "print(f['hardlink'] == grp['new_dataset_name'])\n",
    "\n",
    "# move the dataset back\n",
    "grp.move(\"new_dataset_name\", \"dataset\")\n",
    "\n",
    "# create a soft link that points ot the path \"/mygroup/dataset\"\n",
    "f['softlink'] = h5py.SoftLink(\"/mygroup/dataset\")\n",
    "print(f['softlink'] == grp['dataset'])\n",
    "\n",
    "# get the path\n",
    "softlink = h5py.SoftLink('/some/path')\n",
    "print(softlink.path)\n",
    "\n",
    "# note: nothing happens on the hdf5 side until you assign a soft link\n",
    "# to a name in the file\n",
    "grp.move('dataset', \"new_dataset_name\")\n",
    "dset2 = grp.create_dataset(\"dataset\", (50,))\n",
    "print(f[\"softlink\"] == dset)\n",
    "print(f[\"softlink\"] == dset2)\n",
    "\n",
    "# NOTE: \n",
    "# Soft links are handy if a particular dataset needs to be updated\n",
    "# without breaking all the links to it elsewhere in the file\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external links demo\n",
    "\n",
    "# NOTE: this demo is kinda weird, i get a lot of errors about\n",
    "# not being able to write to files that are already open,\n",
    "# when i run the cell a second time\n",
    "\n",
    "# you can refer to objects in other files!\n",
    "# might be handy when \"friending\" objects\n",
    "\n",
    "f = h5py.File(\"file_with_resource.h5\", \"w\")\n",
    "f.create_group(\"mygroup\")\n",
    "f.close()\n",
    "\n",
    "f2 = h5py.File(\"linking_file.h5\", \"w\")\n",
    "f2['linkname'] = h5py.ExternalLink(\"file_with_resource.h5\", \"mygroup\")\n",
    "\n",
    "grp = f2['linkname'] # weirdness happens here\n",
    "print(grp.name)\n",
    "print(grp.file)\n",
    "print(f2)\n",
    "\n",
    "# the .parent property of the retrieved object doesn't point to \n",
    "# the file where the link is, it points to the root group of the\n",
    "# external file:\n",
    "print(f2['/linkname'].parent == f2['/'])\n",
    "\n",
    "f.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using hdf5's \"get\" to determine object types\n",
    "# get, getclass, getlink\n",
    "\n",
    "# getclass: lets you retrieve the type of an object without opening it\n",
    "# getlink: lets you determine the properties of the link involved\n",
    "\n",
    "f = h5py.File(\"get_demo.h5\", \"w\")\n",
    "\n",
    "f.create_group(\"subgroup\") # these two are hard links\n",
    "f.create_dataset(\"dataset\", (100,))\n",
    "\n",
    "# getclass\n",
    "print(\"--- getclass ---\")\n",
    "for name in f:\n",
    "    print(name, f.get(name, getclass=True))\n",
    "\n",
    "# getlink\n",
    "f[\"softlink\"] = h5py.SoftLink(\"/subgroup\")\n",
    "with h5py.File(\"get_demo_ext.h5\",\"w\") as f2:\n",
    "    f2.create_group(\"egroup\")\n",
    "f[\"extlink\"] = h5py.ExternalLink(\"get_demo_ext.h5\",\"/egroup\")\n",
    "\n",
    "print(\"\\n--- getlink ---\")\n",
    "for name in f:\n",
    "    print(name, f.get(name, getlink=True))\n",
    "\n",
    "# note that INSTANCES of SoftLink and ExternalLink were returned,\n",
    "# complete with path information.\n",
    "    \n",
    "# just tells you the KIND of link involved\n",
    "print(\"\\n--- both ---\")\n",
    "for name in f:\n",
    "    print(name, f.get(name, getlink=True, getclass=True))\n",
    "\n",
    "# note: the HardLink instance doesn't have any other properties (it's a dummy)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 1. \n",
    "# unlike python dicts, you can't directly \n",
    "# overwrite the members of a group, to prevent data loss:\n",
    "\n",
    "f = h5py.File(\"require_demo.h5\",\"w\")\n",
    "\n",
    "f.create_group('x')\n",
    "f.create_group('y')\n",
    "\n",
    "# f.create_group('y') \n",
    "# f['y'] = f['x'] # manual hard link\n",
    "\n",
    "# objects are immediately deleted when you unlink them from a group,\n",
    "# so you have to always do it yourself:\n",
    "\n",
    "del f['y']\n",
    "f['y'] = f['x']\n",
    "\n",
    "# -- 2.\n",
    "# -- using \"require_group\" and \"require_dataset\" -- \n",
    "\n",
    "# if there are many datasets and groups in a file,\n",
    "# it might not be appropriate to overwrite the entire file\n",
    "# every time the code runs.\n",
    "\n",
    "# so use these to check for an existing group or dataset:\n",
    "# create_group -> require_group\n",
    "# create_dataset -> require_dataset\n",
    "\n",
    "f.create_dataset('dataset', (100,), dtype='i')\n",
    "f.require_dataset('dataset', (100,), dtype='i')\n",
    "# f.require_dataset('dataset', (100,), dtype='f') # would fail, wrong type\n",
    "\n",
    "# require_dataset will fail if we find a conflict:\n",
    "# 1. shapes don't match\n",
    "# 2. requested precision (eg int64) is higher than available (eg int32)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration and Containership\n",
    "\n",
    "_Groups_ are actually indexed by HDF5 using a \"B-tree\".\n",
    "\n",
    "They work by taking a collection of items, ordered by some scheme like string name or numeric identifier, and building a tree-like \"index\" to rapidly retrieve an item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating over things will USUALLY be alphabetical,\n",
    "# but hdf5 is really using \"native\" or \"as fast as possible\" retrieval.\n",
    "# if you don't modify the group, the order doesn't change\n",
    "\n",
    "f = h5py.File(\"iterationdemo.h5\",\"w\")\n",
    "f.create_group('b')\n",
    "f.create_group('2')\n",
    "g1 = f.create_group('a')\n",
    "g2 = g1.create_group('sweet')\n",
    "g3 = g2.create_group('path')\n",
    "f.create_dataset('data', (100,))\n",
    "\n",
    "# these two are the same\n",
    "# for key in f:\n",
    "#     print(key, type(key))\n",
    "# for key in f.keys():\n",
    "#     print(key)\n",
    "\n",
    "# print the subgroups of a group\n",
    "for key in g1:\n",
    "    print(key)\n",
    "\n",
    "# return the actual instance\n",
    "for key, val in f.items():\n",
    "    print(key, val, type(val))\n",
    "    \n",
    "print(\"\\n--- check for groups ---\")\n",
    "    \n",
    "# DON'T do this:\n",
    "if 'sweet' in g1: # creates a giant throwaway list\n",
    "    print(\"yeah dog, i'm slow\")\n",
    "\n",
    "# DO this:\n",
    "if 'path' in g2: # uses underlying hdf5 index\n",
    "    print(\"totes bro\")\n",
    "\n",
    "# DO this:\n",
    "if 'a/sweet/path' in f:\n",
    "    print(\"yeah, found it!\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If you’re manipulating POSIX-style strings.\n",
    "# and run into this problem, consider “normalizing” \n",
    "# your paths using the posixpath package: \n",
    "\n",
    "# note also, this cell doesn't work as written, it's just a reminder\n",
    "\n",
    "# grp = f['/1']\n",
    "# path = \"../1\"\n",
    "# import posixpath as pp\n",
    "# path = pp.normpath(pp.join(grp.name, path))\n",
    "# print(path)\n",
    "# if path in grp:\n",
    "#     print(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- multilevel iteration -- the good stuff -- \n",
    "\n",
    "# what if you want to iterate over every single object in the file?\n",
    "# or all objects \"below\" a certain group?\n",
    "\n",
    "# xtra credit: what if I want to *declare* a bunch of groups\n",
    "# based on some pre-arranged list of strings (and datatypes?)\n",
    "\n",
    "# this is done by \"visitor iteration,\" \n",
    "# b/c hdf5 doesn't give you a nice iterable.\n",
    "\n",
    "f = h5py.File(\"visit_test.h5\", \"w\")\n",
    "\n",
    "f.create_dataset(\"top_dataset\", data=1.0)\n",
    "\n",
    "f.create_group(\"top_group_1\")\n",
    "f.create_group(\"top_group_1/subgroup_1\")\n",
    "f.create_dataset(\"top_group_1/subgroup_1/sub_dataset_1\", data=1.0)\n",
    "\n",
    "f.create_group('top_group_2')\n",
    "f.create_dataset(\"top_group_2/sub_dataset_2\", data=1.0)\n",
    "\n",
    "# visit : takes the object name\n",
    "\n",
    "def printname(name):\n",
    "print(name)\n",
    "\n",
    "f.visit(printname)\n",
    "\n",
    "# visit a subgroup only\n",
    "\n",
    "print(\"\\n-- visiting top group 1 only --\")\n",
    "grp = f[\"top_group_1\"]\n",
    "grp.visit(printname)\n",
    "\n",
    "# get a list of every single object in the file:\n",
    "mylist = []\n",
    "f.visit(mylist.append)\n",
    "\n",
    "print(\"\\n-- Multiple links with 'visit' -- \")\n",
    "# if multiple links point to your dataset, when 'visit' supplies\n",
    "# a name, it may not be the one you expect:\n",
    "\n",
    "grp['hardlink'] = f['top_group_2']\n",
    "grp.visit(printname)\n",
    "\n",
    "# the group at /top_group_2 is \"mounted\" in the file at\n",
    "# /top_group_1/hardlink, and visit explores it correctly\n",
    "\n",
    "del grp['hardlink']\n",
    "\n",
    "# this tries to trick visit into visiting sub_dataset_1 twice:\n",
    "print(\"\\n -- try going to same place twice --\")\n",
    "grp['hardlink_to_dataset'] = grp['subgroup_1/sub_dataset_1']\n",
    "grp.visit(printname)\n",
    "\n",
    "# it fails, bc each object in a file will be visited only ONCE\n",
    "\n",
    "# -- more visiting tricks -- \n",
    "def printobj(name, obj):\n",
    "    # this returns a name and an instance,\n",
    "    # we're advised to use it only when we need to check the \n",
    "    # properties of the object (like its type)\n",
    "    print(name, obj)\n",
    "    \n",
    "print(\"\\n-- get name and an instance --\")\n",
    "grp.visititems(printobj)\n",
    "\n",
    "def print_abspath(somegroup, name):\n",
    "    # print *name* as an absolute path\n",
    "    # somegroup: HDF5 base group (*name* is relative to this)\n",
    "    # name: object name relative to *somegroup*\n",
    "    print(posixpath.join(somegroup.name, name))\n",
    "    \n",
    "# print the absolute path of each object in the group\n",
    "print(\"\\n -- print full paths --\")\n",
    "grp.visit(partial(print_abspath, grp))\n",
    "\n",
    "# search: find a dataset that has an attribute w/ a particular value\n",
    "f['top_group_2/sub_dataset_2'].attrs[\"special\"] = 42\n",
    "\n",
    "def find_special(name, obj):\n",
    "    if obj.attrs.get('special') == 42:\n",
    "        return obj\n",
    "\n",
    "print(\"\\n-- looking for a particular thing -- \")\n",
    "out = f.visititems(find_special)\n",
    "print(out)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary style iteration: \n",
    "# because __groups work like dictionaries!__\n",
    "# https://stackoverflow.com/questions/34330283/how-to-differentiate-between-hdf5-datasets-and-groups-with-h5py\n",
    "    \n",
    "# this question is about differentiating GROUPS from DATASETS.\n",
    "    \n",
    "def ds_iterator(g, prefix=''):\n",
    "    for key in g.keys():\n",
    "        item = g[key]\n",
    "        path = '{}/{}'.format(prefix, key)\n",
    "        if isinstance(item, h5py.Dataset): # test for dataset\n",
    "            yield (path, item)\n",
    "        elif isinstance(item, h5py.Group): # test for group (go down)\n",
    "            yield from ds_iterator(item, path)\n",
    "            \n",
    "with h5py.File('iterationdemo.h5', 'r') as f:\n",
    "    for (path, dset) in ds_iterator(f):\n",
    "        print(path, dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying objects\n",
    "\n",
    "f = h5py.File(\"copytest.h5\",\"w\")\n",
    "\n",
    "f.create_group('mygroup')\n",
    "f.create_group('mygroup/subgroup')\n",
    "f.create_dataset(\"mygroup/apples\", (100,))\n",
    "\n",
    "# copying a dataset results in a brand-new dataset,\n",
    "# not a reference or link \n",
    "f.copy('/mygroup/apples', '/oranges')\n",
    "\n",
    "print(f['oranges'] == f['mygroup/apples'])\n",
    "\n",
    "# copy() also correctly handles recursively copying groups:\n",
    "print(\"\\n-- recursive copying -- \")\n",
    "f.copy('mygroup', 'mygroup2')\n",
    "f.visit(printname)\n",
    "\n",
    "# the book SAYS you can also throw Datasets directly into copy,\n",
    "# but I can't get this block to work.\n",
    "# I get: ValueError: Field names only allowed for compound types\n",
    "\n",
    "print(\"\\n -- direct ds copy --\")\n",
    "dset = f['/mygroup/apples']\n",
    "grp = f.create_group(\"/testing\")\n",
    "print(type(dset))\n",
    "# f.copy(dset, f) # copy to file (fails)\n",
    "# f.copy(dset, grp) # copy to group (fails)\n",
    "# f.visit(printname)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more on object comparison\n",
    "\n",
    "f = h5py.File(\"objectdemo.h5\",\"w\")\n",
    "\n",
    "grpx = f.create_group('x')\n",
    "grpy = f.create_group('y')\n",
    "\n",
    "# simple comparison\n",
    "print(grpx == f['x'])\n",
    "print(grpx == grpy)\n",
    "\n",
    "# check python's 'id' function to see if \n",
    "# the objects are ACTUALLY the same\n",
    "print(id(grpx) == id(f['x']))\n",
    "\n",
    "# but the hash is the same:\n",
    "print(hash(grpx) == hash(f['x']))\n",
    "      \n",
    "# also note: the .file property will be equal to the group\n",
    "# when we're in the root directory:\n",
    "print(f == f['/'])\n",
    "\n",
    "# check if an object is \"alive\" or \"dead\":\n",
    "print(bool(grpx))\n",
    "f.close()\n",
    "print(bool(grpx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6: Metadata and Attributes\n",
    "\n",
    "_Attributes_ are bits of metadata you can attach to all HDF5 objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attribute demo\n",
    "\n",
    "# NOTE: w/ default settings, MAX attribute size is 64 KB.\n",
    "\n",
    "f = h5py.File(\"attrsdemo.h5\", \"w\")\n",
    "\n",
    "dset = f.create_dataset(\"dataset\",(100,))\n",
    "print(dset.attrs)\n",
    "\n",
    "# create a new attributes\n",
    "dset.attrs['title'] = \"dataset from third round of experiments\"\n",
    "dset.attrs['sample_rate'] = 100e6 # clock freq\n",
    "dset.attrs['run_id'] = 144\n",
    "# dset.attrs[\"fail\"] = {'a':1, \"b\":2} # nope, fails.\n",
    "dset.attrs['fail'] = [1,2,\"b\",4] # succeeds, but is converted to np array\n",
    "\n",
    "print(dset.attrs['title'])\n",
    "print(type(dset.attrs['title'])) # gives a string\n",
    "print(dset.attrs['fail'])\n",
    "print(type(dset.attrs['fail']))\n",
    "\n",
    "# iterate over attributes like a dictionary\n",
    "print([x for x in dset.attrs])\n",
    "\n",
    "# overwriting attributes is allowed!\n",
    "dset.attrs[\"another_id\"] = 42\n",
    "dset.attrs[\"another_id\"] = 100\n",
    "\n",
    "# can also delete them:\n",
    "del dset.attrs[\"another_id\"]\n",
    "\n",
    "# can iterate over them\n",
    "print(\"\\n--attrs--\")\n",
    "for name, val in dset.attrs.items():\n",
    "    print(name, val)\n",
    "    \n",
    "# hdf5 also has a 'get' for attrs:\n",
    "print(dset.attrs.get('run_id'))\n",
    "\n",
    "# if we really needed to store a big np array as an attribute,\n",
    "# we could store the data in an extra dataset, then link to it\n",
    "# using the reference:\n",
    "\n",
    "ones_dset = f.create_dataset('ones_data',data=np.ones((100,100)))\n",
    "dset.attrs['ones'] = ones_dset.ref\n",
    "print(dset.attrs['ones'])\n",
    "\n",
    "# some fortran stuff can't handle variable length strings\n",
    "# as attributes (like the one above), so numpy has a fixed\n",
    "# length version\n",
    "dset.attrs['title_fixed'] = np.string_(\"Another title\")\n",
    "\n",
    "# can also store unicode strings\n",
    "dset.attrs[\"yet another title\"] = u'String with accent (\\u00E9)'\n",
    "\n",
    "# if you INSIST on storing a python object, you have to pickle:\n",
    "import pickle\n",
    "pickled_object = pickle.dumps({'key':42}, protocol=0)\n",
    "dset.attrs['object'] = pickled_object\n",
    "obj = pickle.loads(dset.attrs['object'])\n",
    "print(obj)\n",
    "\n",
    "# write stuff to the file\n",
    "f.flush()\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "h5ls -vlr attrsdemo.h5\n",
    "# some types might be \"native\", but they can probably be converted\n",
    "# to numpy types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicit typing\n",
    "# if \"native\" type isn't good enough\n",
    "\n",
    "f = h5py.File('attrs_create.h5',\"w\")\n",
    "\n",
    "# specify the type of the attr\n",
    "dset = f.create_dataset('dataset', (100,))\n",
    "dset.attrs.create(\"two_byte_int\", 190, dtype='i2')\n",
    "dset.attrs['two_byte_int']\n",
    "\n",
    "# for strings, you have to be more careful.\n",
    "\n",
    "# use whatever h5py thinks you can use\n",
    "dset.attrs['strings'] = [\"Hello\",\"Another string\"]\n",
    "print(dset.attrs['strings'])\n",
    "\n",
    "# manually specify a variable length string\n",
    "dt = h5py.special_dtype(vlen=str)\n",
    "dset.attrs.create('more_strings', [\"Hello\",\"Another string\"], dtype=dt)\n",
    "print(dset.attrs[\"more_strings\"])\n",
    "\n",
    "# NOTE: the book says that these will be different datatypes, \n",
    "# but that is NOT what I'm getting.  Both are coming out as\n",
    "# \"variable-length null-terminated UTF-8\" strings.\n",
    "\n",
    "# modify an attr has a special function\n",
    "dset.attrs.modify('two_byte_int', 33) # this might fail if the type changes\n",
    "dset.attrs[\"two_byte_int\"] = 34.0 # or you can just overwrite\n",
    "print(dset.attrs['two_byte_int'])\n",
    "\n",
    "# apparently when you modify attributes,\n",
    "# you have to call flush() to get them to go into the file\n",
    "f.flush()\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "h5ls -vlr attrs_create.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7: Types\n",
    "\n",
    "HDF5 supports a lot of datatypes, in some cases ones that NumPy doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"types_demo.h5\",\"w\")\n",
    "\n",
    "# -- integers & floats -- \n",
    "dset = f.create_dataset('smallint', (10,), dtype=np.int8)\n",
    "dset[0] = 300 # this is too big for this dtype, so it rolls over\n",
    "print(dset[0])\n",
    "\n",
    "# same thing happens in numpy:\n",
    "a = np.zeros((10,), dtype=np.int8)\n",
    "a[0] = 300\n",
    "print(a[0]) # huh, book gets -44, I get 44.  yay versions!\n",
    "\n",
    "# trick, a 'half-precision' float: \n",
    "# useful when a 16-bit int isn't quite enough\n",
    "# i.e. your values are between 10^{-8} and 60,000\n",
    "# note these are only for storage, trying to do math on them will \n",
    "# result in casting.  use Dataset.read_direct or astype\n",
    "dset = f.create_dataset(\"half_float\", (100,100,100), dtype=np.float16)\n",
    "a = dset[...]\n",
    "a = a.astype(np.float32)\n",
    "\n",
    "\n",
    "# -- fixed length strings -- \n",
    "# NOTE: this is different from the book, b/c python3:\n",
    "# see here for some more info:\n",
    "# https://github.com/h5py/h5py/issues/289\n",
    "\n",
    "dt = np.dtype(\"S10\")\n",
    "dset = f.create_dataset(\"fixed_string\",(100,), dtype=dt)\n",
    "dset[0] = \"Hello\".encode(\"utf-8\") # different from book, you have to encode.\n",
    "dset[0] = \"thisstringhasmorethan10chars\".encode(\"utf-8\")\n",
    "print(dset[0])\n",
    "\n",
    "\n",
    "# -- variable length strings (use special h5py datatype) -- \n",
    "dt = h5py.special_dtype(vlen=str)\n",
    "print(dt) # different from book, but is still an object\n",
    "print(dt.kind) # yep, 'O' for object\n",
    "\n",
    "dset = f.create_dataset(\"vlen_dataset\", (100,), dtype=dt)\n",
    "dset[0] = \"Hello\" # rad, i didn't have to encode this\n",
    "dset[1] = np.string_(\"Hello2\")\n",
    "dset[3] = \"X\"*30 # lol, book typo?\n",
    "\n",
    "out = dset[0]\n",
    "print(out, type(out))\n",
    "print(dset[0:5]) # i see the blank space from the book typo\n",
    "\n",
    "# note, you don't always get the same type out as you put in\n",
    "out = dset[0:1]\n",
    "print(out.dtype) # just \"object\"\n",
    "\n",
    "# NOTE: Python3 uses Unicode strings for everything!\n",
    "# dt = h5py.special_dtype(vlen=str) # UTF-8 (default)\n",
    "# dt = h5py.special_dtype(vlen=bytes) # ASCII \n",
    "\n",
    "\n",
    "# -- Compound types -- \n",
    "# bundle stuff together w different types, like a C struct or a table\n",
    "\n",
    "dt = np.dtype([(\"temp\",np.float),(\"pressure\",np.float),(\"wind\",np.float)])\n",
    "a = np.zeros((100,), dtype=dt)\n",
    "\n",
    "# numpy side:\n",
    "# when you access a single element, you get back an object\n",
    "# that supports dict-style access on the field names:\n",
    "out = a['temp']\n",
    "out = a[0]\n",
    "print(out, out[\"temp\"])\n",
    "\n",
    "# hdf5 side:\n",
    "dset = f.create_dataset(\"compound\",(100,), dtype=dt)\n",
    "out = dset[\"temp\",\"pressure\"]\n",
    "print(out.shape)\n",
    "print(out.dtype)\n",
    "out = dset[\"temp\", 90:100] # mix names & slices, get last 10 temp points\n",
    "print(out.dtype)\n",
    "\n",
    "# set all temps we just read to a new value and write\n",
    "out[...] = 98.6\n",
    "dset[\"temp\", 90:100] = out\n",
    "\n",
    "# complex dtypes (not in hdf5, but there is a convention)\n",
    "dset = f.create_dataset(\"single_complex\",(100,), dtype='c8')\n",
    "# can check w/ h5ls to see.\n",
    "\n",
    "\n",
    "# -- Enumerated types -- \n",
    "# aka. integer datatypes for which certain values are associated \n",
    "# with TEXT TAGS. \n",
    "mapping = {\"red\":0, \"green\":1, \"blue\":2}\n",
    "dt = h5py.special_dtype(enum=(np.int8, mapping))\n",
    "dset = f.create_dataset(\"enum\",(100,), dtype=dt)\n",
    "\n",
    "print(dset[0], dset[0].dtype) # don't get the \"extras\" from the enum here\n",
    "\n",
    "\n",
    "# -- booleans -- \n",
    "# there isn't a native hdf5 boolean type, so h5py provides one\n",
    "# can check w/ h5ls\n",
    "f.create_dataset(\"bool\",(100,), dtype=np.bool)\n",
    "\n",
    "\n",
    "# -- array -- \n",
    "# good choice when you want to store multiple values of the\n",
    "# SAME TYPE in a single element.\n",
    "# but be careful, it's another case where \n",
    "# dset[...].dtype != dset.dtype  (i.e. you get back something else!)\n",
    "\n",
    "dt = np.dtype(\"(2,2)f\")\n",
    "print(dt)\n",
    "dset = f.create_dataset(\"array\", (100,), dtype=dt)\n",
    "print(dset.shape)\n",
    "out = dset[0]\n",
    "print(type(out)) # yup, just a np array now\n",
    "\n",
    "# if we were to create a native numpy array with our type\n",
    "# it would get \"eaten\":\n",
    "a = np.zeros((100,), dtype=dt)\n",
    "print(a.dtype, a.shape)\n",
    "\n",
    "# the book says the array type is good when it's an element\n",
    "# of a compound type.  for example:\n",
    "# \"if we had an experiment that reported an integer timestamp\n",
    "# along with the output from a 2x2 light sensor, one choice\n",
    "# for a data type would be:\"\n",
    "dt_timestamp = np.dtype('uint64')\n",
    "dt_sensor = np.dtype('(2,2)f')\n",
    "dt = np.dtype([ (\"time\",dt_timestamp), (\"sensor\",dt_sensor) ])\n",
    "\n",
    "# now store and retrieve individual outputs from the experiment:\n",
    "dset = f.create_dataset('mydata', (100,), dtype=dt)\n",
    "dset[\"time\",0] = time.time()\n",
    "\n",
    "# this example from the book doesn't work. \n",
    "# I guess it's a python3 thing.\n",
    "# I get this error:\n",
    "# ValueError: When changing to a larger dtype, its size must be \n",
    "# a divisor of the total size in bytes of the last axis of the array.\n",
    "# val = np.asarray([[1,2],[3,4]])\n",
    "# print(val, type(val))\n",
    "# dset[\"sensor\",0] = val # <-- fails\n",
    "# out = dset[0]\n",
    "# print(out)\n",
    "# print(out[\"sensor\"])\n",
    "\n",
    "\n",
    "# -- dates and times --\n",
    "# notes: hdf5 doesn't ever use its \"datetime\" type,\n",
    "# so date/time handling is \"ad hoc\"\n",
    "\n",
    "# unix time (1 sec precision)\n",
    "timestamp = np.dtype('u8')\n",
    "\n",
    "# unix time (< sec precision)\n",
    "print(time.time())\n",
    "\n",
    "# ISO format:\n",
    "print(datetime.datetime.now().isoformat())\n",
    "\n",
    "# NOTE: we're not saving the time zone, daylight savings, or anything\n",
    "# else here.  so be careful!  maybe use attrs if you need to.\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8: References, Types, and Dimension Scales\n",
    "\n",
    "\"Some of the best features in HDF5 are those that help you to express _relationships_ between pieces of your data.\"\n",
    "\n",
    "3 main things:\n",
    "- _References_ (HDF5's pointer type) are a great way to store _links to objects_ as data.\n",
    "- _Named types_ let you enforce type consistency across datasets.\n",
    "- _Dimension Scales_ (an HDF5 standard), let you attach _physically meaningful axes_ to your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object references\n",
    "\n",
    "# links in a group serve to locate objects.\n",
    "# REFERENCES are another mechanism to do this,\n",
    "# and they can be stored AS DATA, in attrs and datasets.\n",
    "\n",
    "f = h5py.File(\"refs_demo.h5\",\"w\")\n",
    "\n",
    "grp1 = f.create_group(\"group1\")\n",
    "grp2 = f.create_group(\"group2\")\n",
    "dset = f.create_dataset(\"mydata\", shape=(100,))\n",
    "\n",
    "# get an object's reference (the pointer to the object in the file)\n",
    "print(grp1.ref)\n",
    "\n",
    "# dereference\n",
    "out = f[grp1.ref]\n",
    "print(out == grp1)\n",
    "\n",
    "# check instance\n",
    "print(isinstance(grp1.ref, h5py.Reference))\n",
    "\n",
    "# refs are local to their file\n",
    "# with h5py.File(\"anotherfile.h5\",\"w\") as f2:\n",
    "#     out = f2[grp1.ref] # ValueError: unable dereference object\n",
    "      \n",
    "# Unlike links, references can be stored as data:\n",
    "# they are \"unbreakable links.\"\n",
    "\n",
    "grp1.attrs['dataset'] = dset.name\n",
    "out = f[grp1.attrs['dataset']]\n",
    "print(out == dset)\n",
    "\n",
    "# if we rename the dataset, the above will break:\n",
    "f.move(\"mydata\",\"mydata2\")\n",
    "# out = f[grp1.attrs['dataset']] # KeyError: \"unable to open object\"\n",
    "\n",
    "# instead, use an object reference\n",
    "grp1.attrs['dataset'] = dset.ref\n",
    "out = f[grp1.attrs['dataset']]\n",
    "print(out == dset) # this time, True\n",
    "\n",
    "# NOTE:\n",
    "# when you open an object by dereferencing, every now and then it's \n",
    "# possible HDF5 won't be able to figure out the object's name.\n",
    "# In that case, obj.name will return None.\n",
    "\n",
    "# storing a reference as data requires a special hdf5 datatype:\n",
    "dt = h5py.special_dtype(ref=h5py.Reference)\n",
    "# print(dt.kind) # \"O\" for object\n",
    "\n",
    "ref_dset = f.create_dataset(\"references\",(10,), dtype=dt)\n",
    "out = ref_dset[0]\n",
    "print(out)\n",
    "\n",
    "# derefrence the null reference: you get a ValueError\n",
    "# print(f[out])\n",
    "\n",
    "# check for a null reference with a bool\n",
    "print(bool(out), bool(grp1.ref))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region references\n",
    "\n",
    "# \"one of the coolest features of HDF5\"\n",
    "\n",
    "# maybe you want to store a region of interest (ROI) of a dataset,\n",
    "# so during later analysis you don't have to process the whole thing!\n",
    "\n",
    "# they store a slice argument for later use!\n",
    "\n",
    "f = h5py.File('refs2.h5','w')\n",
    "\n",
    "dt = h5py.special_dtype(ref=h5py.Reference)\n",
    "# dset = f.create_dataset(\"references\",(10,), dtype=dt)\n",
    "dset = f.create_dataset('mydata', shape=(100,))\n",
    "\n",
    "print(dset.name)\n",
    "print(dset.shape)\n",
    "print(dset.regionref) # <-- this thing\n",
    "\n",
    "# create a region ref\n",
    "ref_out = dset.regionref[10:90]\n",
    "print(ref_out)\n",
    "\n",
    "# the book says these exist, but they don't work for me\n",
    "# dset.regionref.shape(ref_out)(100,) # same as parent\n",
    "# dset.regionref.selection(ref_out)(80,) # shape of selection\n",
    "\n",
    "# if you have a region reference (you read from a file, presumably)\n",
    "# you can use it to slice the dataset on retrieval:\n",
    "data = dset[ref_out]\n",
    "\n",
    "print(data)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fancy indexing\n",
    "\n",
    "# methods like Boolean arrays will always have a 1-D shape\n",
    "\n",
    "f = h5py.File(\"fancy_indexing.h5\",\"w\")\n",
    "\n",
    "# numpy side\n",
    "arr = np.random.random((3,3))\n",
    "idx = np.where(arr > 0.5) # returns a tuple, only indexes\n",
    "print(type(idx),\"\\n\",idx) \n",
    "\n",
    "# hdf5 side\n",
    "dset_random = f.create_dataset(\"small_example\",(3,3))\n",
    "dset_random[...] = arr\n",
    "idx = dset_random[...] > 0.5 # returns a boolean array, same shape as orig\n",
    "print(type(idx),\"\\n\", idx)\n",
    "\n",
    "# create a region reference from this selection:\n",
    "random_ref = dset_random.regionref[idx]\n",
    "\n",
    "# access it\n",
    "print(dset_random.regionref.selection(random_ref))\n",
    "\n",
    "data = dset_random[random_ref]\n",
    "print(data)\n",
    "\n",
    "# huhhhh, this returns a 1d array, not a (3,3).\n",
    "\n",
    "# the book says the list-based selection is returned as a 1D\n",
    "# array, following \"C order\": the selection avances through the last\n",
    "# index, then the next to last, and so on.\n",
    "# \"this is a limitation of the HDF5 library.\"\n",
    "\n",
    "# last trick: if you have a region reference, you can use\n",
    "# it as an OBJECT reference to retrieve the whole dataset:\n",
    "\n",
    "# imagine you didn't have this, but just the file and a regionref\n",
    "dset = f.create_dataset('mydata', shape=(100,)) \n",
    "\n",
    "ref_out = dset.regionref[10:90]\n",
    "\n",
    "print(f[ref_out])\n",
    "\n",
    "# and the actual subset:\n",
    "selected_data = f[ref_out][ref_out]\n",
    "\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 named type \"mytype\" (dtype <f4)>\n",
      "float32\n",
      "/mytype\n",
      "<HDF5 group \"/\" (1 members)>\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "# named types\n",
    "\n",
    "# imagine you had many datasets in a file (like a bunch of images),\n",
    "# and you wanted to be sure every image has exactly the same type.\n",
    "# HDF5 lets you save the datatype to the FILE.\n",
    "# when you call create_dataset, you supply the stored type\n",
    "# and HDF5 will \"link\" the type to the brand new dataset.\n",
    "\n",
    "f = h5py.File(\"named_types.h5\",\"w\")\n",
    "\n",
    "f[\"mytype\"] = np.dtype('float32')\n",
    "\n",
    "out = f['mytype']\n",
    "out.attrs['info'] = \"this is my datatype\"\n",
    "\n",
    "print(out)\n",
    "print(out.dtype)\n",
    "print(out.name)\n",
    "print(out.parent)\n",
    "\n",
    "# linking to named types\n",
    "\n",
    "dset = f.create_dataset(\"typedemo\",(100,), dtype=f['mytype']) # a link \n",
    "\n",
    "# for attributes, you have to supply the type with create()\n",
    "f.attrs.create(\"attribute_demo\", 1.0, dtype=f['mytype'])\n",
    "\n",
    "# you can't modify named types.  and if you unlink them, they still\n",
    "# stick around until every object they're associated with is deleted.\n",
    "del f['mytype']\n",
    "f['mytype'] = np.dtype('int16')\n",
    "dset = f['typedemo']\n",
    "print(dset.dtype) # <-- nope, still float32.\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Dimensions of HDF5 object at 4536618856>\n",
      "CLASS  :  b'DIMENSION_SCALE'\n",
      "NAME  :  b'Simulation X (North) axis'\n",
      "<HDF5 dataset \"scale_x\": shape (100,), type \"<f8\">\n",
      "[     0.  10000.  20000.  30000.  40000.  50000.  60000.  70000.  80000.\n",
      "  90000. 100000. 110000. 120000. 130000. 140000. 150000. 160000. 170000.\n",
      " 180000. 190000. 200000. 210000. 220000. 230000. 240000. 250000. 260000.\n",
      " 270000. 280000. 290000. 300000. 310000. 320000. 330000. 340000. 350000.\n",
      " 360000. 370000. 380000. 390000. 400000. 410000. 420000. 430000. 440000.\n",
      " 450000. 460000. 470000. 480000. 490000. 500000. 510000. 520000. 530000.\n",
      " 540000. 550000. 560000. 570000. 580000. 590000. 600000. 610000. 620000.\n",
      " 630000. 640000. 650000. 660000. 670000. 680000. 690000. 700000. 710000.\n",
      " 720000. 730000. 740000. 750000. 760000. 770000. 780000. 790000. 800000.\n",
      " 810000. 820000. 830000. 840000. 850000. 860000. 870000. 880000. 890000.\n",
      " 900000. 910000. 920000. 930000. 940000. 950000. 960000. 970000. 980000.\n",
      " 990000.]\n"
     ]
    }
   ],
   "source": [
    "# dimension scales\n",
    "\n",
    "# in the real world, we have units attached to things!\n",
    "\n",
    "f = h5py.File(\"dim_scales.h5\",\"w\")\n",
    "\n",
    "dset = f.create_dataset('temperatores', (100,100,100), dtype='f')\n",
    "\n",
    "# we might start by trying to store important info with the attrs:\n",
    "dset.attrs[\"temp_units\"] = \"C\"\n",
    "dset.attrs[\"steps\"] = [10000,10000,100] # measurement resolution, say\n",
    "dset.attrs[\"axes\"] = [\"x\", \"y\", \"z\"]\n",
    "\n",
    "# but that's chump change.  \n",
    "# HDF5 has \"DIMENSION SCALES\", which you can read instead:\n",
    "\n",
    "for name in dset.attrs:\n",
    "    del dset.attrs[name]\n",
    "\n",
    "print(dset.dims) # <-- entry point to a separate dataset w/ metadata\n",
    "\n",
    "f.create_dataset(\"scale_x\", data=np.arange(100)*10e3)\n",
    "f.create_dataset(\"scale_y\", data=np.arange(100)*10e3)\n",
    "f.create_dataset(\"scale_z\", data=np.arange(100)*100.0)\n",
    "\n",
    "dset.dims.create_scale(f['scale_x'], \"Simulation X (North) axis\")\n",
    "dset.dims.create_scale(f['scale_y'], \"Simulation Y (East) axis\")\n",
    "dset.dims.create_scale(f['scale_z'], \"Simulation Z (Vertical) axis\")\n",
    "\n",
    "for key, val in f['scale_x'].attrs.items():\n",
    "    print(key, \" : \", val)\n",
    "\n",
    "# huh.  all it did is just attach a few attributes w/ \n",
    "# standardized names and values.\n",
    "\n",
    "# now let's associate the scales to the dataset.\n",
    "\n",
    "dset.dims[0].attach_scale(f['scale_x'])\n",
    "dset.dims[1].attach_scale(f['scale_y'])\n",
    "dset.dims[2].attach_scale(f['scale_z'])\n",
    "\n",
    "dset.dims[0].label = 'x'\n",
    "dset.dims[1].label = 'y'\n",
    "dset.dims[2].label = 'z'\n",
    "\n",
    "# dims[N] is a proxy keeping track of which dimension scales are\n",
    "# attached to the first axis of the dataset\n",
    "# people who create plots w/ an axis on every side can use multiple scales\n",
    "\n",
    "print(dset.dims[0][0])\n",
    "print(dset.dims[0][0][...]) # <-- get the actual axis values\n",
    "\n",
    "# so by saving the actual axis values, we can actually handle \n",
    "# variable length binning and other stuff that just using \"attrs\"\n",
    "# wouldn't have allowed us to do!\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 9: Parallelizing HDF5, aka \"Concurrency\"\n",
    "\n",
    "Python includes a single master lock that governs access to the interpreter’s functions, called the Global Interpreter Lock or GIL. This lock serializes access from multiple threads to basic resources like object reference counting. You can have as many threads as you like in a Python program, but only one at a time can use the interpreter.\n",
    "\n",
    "Access to HDF5 is serialized using locking, so only _one thread at a time_ can work with the library.\n",
    "\n",
    "`h5py` is _thread-safe_, in that you can safely share objects between threads without corruption, and there's no global state that lets one thread stomp on another.\n",
    "\n",
    "NOTE: The book says there are a bunch of problems with `multiprocessing` with HDF5 as of October 2013.  I went to the h5py github and downloaded an example to this folder, `multiprocessing_example.py`, that runs beautifully and displays a great fractal.  If you really need to be sure you're doing it right, check both.\n",
    "\n",
    "The issue with `multiprocessing` is that _new processes inherit the state of the HDF5 library from the parent process_.  You end up with multiple processes \"fighting\" each other for the same file.  Some things to try:\n",
    "\n",
    "1. Do all your file I/O in the main process, but don’t have files open when you invoke the multiprocessing features.  \n",
    "2. Multiple subprocesses can safely read from the same file, but only open it once the new process has been created.  \n",
    "3. Have each subprocess write to a different file, and merge them when finished.\n",
    "\n",
    "Collette, Andrew. Python and HDF5: Unlocking Scientific Data . O'Reilly Media. Kindle Edition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fake data taking ...\n",
      "huzzah\n"
     ]
    }
   ],
   "source": [
    "# first example : `threading` module\n",
    "# \n",
    "# create a single shared HDF5 file and two threads that do computation,\n",
    "# and write to the file.  Access is managed with `threading.RLock`.\n",
    "\n",
    "import threading\n",
    "import random\n",
    "\n",
    "f = h5py.File(\"thread_demo.h5\", \"w\")\n",
    "dset = f.create_dataset(\"data\",(2,1024),dtype='f')\n",
    "\n",
    "lock = threading.RLock()\n",
    "\n",
    "class ComputeThread(threading.Thread):\n",
    "    def __init__(self, axis):\n",
    "        # one thread does dset[0,:], the other dset[1,:]\n",
    "        self.axis = axis \n",
    "        threading.Thread.__init__(self)\n",
    "        \n",
    "    def run(self):\n",
    "        # perform a series of (simulated) computations \n",
    "        # and save to dataset.\n",
    "        for idx in range(1024):\n",
    "            rando_number = random.random()*0.01\n",
    "            time.sleep(rando_number) # do computation\n",
    "\n",
    "            # magic step\n",
    "            with lock:\n",
    "                dset[self.axis, idx] = rando_number # save to dataset\n",
    "                \n",
    "thread1 = ComputeThread(0)\n",
    "thread2 = ComputeThread(1)\n",
    "\n",
    "print(\"Running fake data taking ...\")\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "\n",
    "# wait until both threads have finished\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "\n",
    "print(\"huzzah\")\n",
    "\n",
    "f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HELLO', 'SOME', 'WORDS'] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# second example:\n",
    "# using the `multiprocessing` module\n",
    "\n",
    "# distribute the work among worker processes\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# a quick first example\n",
    "p = Pool(2) # creates a 2-process pool\n",
    "words_in = ['hello','some','words']\n",
    "words_out = p.map(str.upper, words_in) # <- call the workers w/ a function\n",
    "print(words_out, type(words_out))\n",
    "\n",
    "# -------\n",
    "# suppose we have a file containing a 1d dataset of coordinate pairs,\n",
    "# and we need to compute their distance from the origin.\n",
    "# this is good to parallelize because each computation doesn't\n",
    "# depend on the others.\n",
    "\n",
    "with h5py.File('coords.h5','w') as f:\n",
    "    dset = f.create_dataset('coords',(1000,2), dtype='f4')\n",
    "    dset[...] = np.random.random((1000,2))\n",
    "    \n",
    "def distance(arr):\n",
    "    # compute distance from origin to the point\n",
    "    # arr is a shape (2,) array\n",
    "    return np.sqrt(np.sum(arr**2))\n",
    "\n",
    "# Load data and close the input file\n",
    "with h5py.File('coords.h5', 'r') as f:\n",
    "    data = f['coords'][...]\n",
    "\n",
    "# run in \"parallel\", but the file IO is explicitly in the main process\n",
    "p = Pool(4)\n",
    "result = np.array(p.map(distance, data))\n",
    "\n",
    "with h5py.File(\"coords.h5\") as f:\n",
    "    f['distances'] = result\n",
    "    \n",
    "# \"doing anything more complex with multiprocessing and HDF5\n",
    "# gets complicated. Your process can't all access the same file.\"\n",
    "\n",
    "# -- I'm going to skip the second example here, which\n",
    "# just calclulates the same thing with blocks of 100 numbers\n",
    "# instead of single values, spits them all to different files,\n",
    "# and recombines.  ugh, gross.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPI and Parallel HDF5\n",
    "\n",
    "\"What if there were a way to share a single file between processes, automatically synchronizing reads and writes?  We would get around the limitations on passing open files to child processes.\n",
    "\n",
    "MPI-based applications work by launching _multiple parallel instances of the Python interpreter_.  The key difference compared to `multiprocessing` is that the processes are _peers_, unlike the child processes used for `Pool`.\n",
    "\n",
    "All file access has to be coordinated though the MPI library as well. If not, multiple processes would “fight” over the same file on disk. Thankfully, HDF5 itself handles nearly all the details involved with this. All you need to do is open shared files with a special driver, and follow some constraints for data consistency.\n",
    "\n",
    "For more examples, look at http://mpi4py.scipy.org/\n",
    "\n",
    "Collette, Andrew. Python and HDF5: Unlocking Scientific Data . O'Reilly Media. Kindle Edition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: to install prereqs, I just did:\n",
    "# $ brew install mpich\n",
    "# $ pip install mpi4py\n",
    "\n",
    "# i can come back to this later, if i feel like it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A: Single-Write, Multiple Reader Mode (SWMR)\n",
    "\n",
    "This isn't in the Kindle version of the book I got, but it's an important new feature.\n",
    "\n",
    "Now I'll work off of the docs:\n",
    "\n",
    "http://docs.h5py.org/en/stable/swmr.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B: Virtual Datasets (VDS)\n",
    "\n",
    "Working off the docs here too:\n",
    "\n",
    "http://docs.h5py.org/en/stable/vds.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
